{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Preparations\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fa6f9982f0dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import scipy.io as sio\n",
    "\n",
    "sys.path.append('../..');\n",
    "from spyrit.learning.model_Had_DCAN import *\n",
    "from spyrit.learning.nets import *\n",
    "from spyrit.misc.disp import *\n",
    "from spyrit.misc.metrics import *\n",
    "\n",
    "\n",
    "\n",
    "import tabulate\n",
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions to load the experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mat_data_index(expe_data, nflip, lambda_i = 548):\n",
    "    F_pos = sio.loadmat(expe_data+\"_{}_100_pos_data.mat\".format(nflip));\n",
    "    F_neg = sio.loadmat(expe_data+\"_{}_100_neg_data.mat\".format(nflip));\n",
    "    F_spectro = F_pos[\"spec\"][0][0][0];\n",
    "    F_spectro = F_spectro[0,:];\n",
    "    lambda_indices = np.where(np.abs(F_spectro-lambda_i)<1);\n",
    "    num_channel = lambda_indices[0][0];\n",
    "    F_data_pos = F_pos[\"F_WT_lambda_pos\"];\n",
    "    F_data_neg = F_neg[\"F_WT_lambda_neg\"];\n",
    "    F_pos = F_data_pos[:,:,num_channel];\n",
    "    F_neg = F_data_neg[:,:,num_channel];\n",
    "    if (2**16-1 in F_pos) or (2**16-1 in F_neg):\n",
    "        warnings.warn(\"Warning, Saturation!\", UserWarning)\n",
    "    F_pos = F_pos.astype(\"int64\");\n",
    "    F_neg = F_neg.astype(\"int64\");\n",
    "    return F_pos, F_neg;\n",
    "\n",
    "\n",
    "def read_mat_data(expe_data, nflip, lambda_min = 460, lambda_max = 700):\n",
    "    F_pos = sio.loadmat(expe_data+\"_{}_100_pos_data.mat\".format(nflip));\n",
    "    F_neg = sio.loadmat(expe_data+\"_{}_100_neg_data.mat\".format(nflip));\n",
    "    F_data_pos = F_pos[\"F_WT_lambda_pos\"];\n",
    "    F_data_neg = F_neg[\"F_WT_lambda_neg\"];\n",
    "    F_spectro = F_pos[\"spec\"][0][0][0];\n",
    "    F_spectro = F_spectro[0,:];\n",
    "    F_pos = F_data_pos[:,:,F_spectro>lambda_min];\n",
    "    F_neg = F_data_neg[:,:,F_spectro>lambda_min];\n",
    "    F_spectro = F_spectro[F_spectro>lambda_min];\n",
    "    F_pos = F_pos[:,:,F_spectro<lambda_max];\n",
    "    F_neg = F_neg[:,:,F_spectro<lambda_max];\n",
    "    F_pos = np.sum(F_pos, axis=2);\n",
    "    F_pos = F_pos.astype(\"int64\");\n",
    "    F_neg = np.sum(F_neg, axis=2);\n",
    "    F_neg = F_neg.astype(\"int64\");\n",
    "    return F_pos, F_neg;\n",
    "\n",
    "def read_mat_data_proc(expe_data, nflipi, lamda_min=460, lambda_max = 700):\n",
    "    F = sio.loadmat(expe_data+\"_{}_100_data.mat\".format(nflip));\n",
    "    F_data = F[\"F_WT_lambda\"];\n",
    "    F_spectro = F[\"spec\"][0][0][0];\n",
    "    F_spectro = F_spectro[0,:];\n",
    "    F = F_data[:,:,F_spectro>lamdba_min];\n",
    "    F_spectro = F_spectro[F_spectro>lambda_min];\n",
    "    F = F[:,:,F_spectro<lambda_max];\n",
    "    F = np.sum(F , axis=2);\n",
    "    return F;\n",
    "\n",
    "def load_data_list_index(expe_data,nflip, CR, K, Perm, img_size, num_channel = 548):\n",
    "    even_index = range(0,2*CR,2);\n",
    "    odd_index = range(1,2*CR,2);\n",
    "    m_list = [];\n",
    "    for i in range(len(nflip)):\n",
    "        F_pos , F_neg = read_mat_data_index(expe_data[i],nflip[i], num_channel);\n",
    "        F_pos = F_pos;\n",
    "        F_neg = F_neg;    \n",
    "        f_pos = np.reshape(F_pos, (img_size**2,1));\n",
    "        f_neg = np.reshape(F_neg, (img_size**2,1));\n",
    "        f_re_pos = np.dot(Perm, f_pos);\n",
    "        f_re_neg = np.dot(Perm, f_neg);\n",
    "        m = np.zeros((2*CR,1));\n",
    "        m[even_index] = f_re_pos[:CR];\n",
    "        m[odd_index] = f_re_neg[:CR];\n",
    "        m = torch.Tensor(m);\n",
    "        m = m.view(1,1,2*CR);\n",
    "        m = m.to(device);\n",
    "        m_list.append(m);\n",
    "    return m_list\n",
    "\n",
    "def load_data_list(expe_data,nflip, CR, K, Perm, img_size, lambda_min = 460, lambda_max = 700):\n",
    "    even_index = range(0,2*CR,2);\n",
    "    odd_index = range(1,2*CR,2);\n",
    "    m_list = [];\n",
    "    for i in range(len(nflip)):\n",
    "        F_pos , F_neg = read_mat_data(expe_data[i],nflip[i], lambda_min, lambda_max);\n",
    "        F_pos = 1/K*F_pos;\n",
    "        F_neg = 1/K*F_neg;    \n",
    "        f_pos = np.reshape(F_pos, (img_size**2,1));\n",
    "        f_neg = np.reshape(F_neg, (img_size**2,1));\n",
    "        f_re_pos = np.dot(Perm, f_pos);\n",
    "        f_re_neg = np.dot(Perm, f_neg);\n",
    "        m = np.zeros((2*CR,1));\n",
    "        m[even_index] = f_re_pos[:CR];\n",
    "        m[odd_index] = f_re_neg[:CR];\n",
    "        m = torch.Tensor(m);\n",
    "        m = m.view(1,1,2*CR);\n",
    "        m = m.to(device);\n",
    "        m_list.append(m);\n",
    "    return m_list\n",
    "  \n",
    "def ground_truth_list_index(expe_data,nflip, H, img_size, num_channel=548):\n",
    "    gt_list = [];\n",
    "    max_list = [];\n",
    "    for i in range(len(nflip)):\n",
    "        F_pos , F_neg = read_mat_data_index(expe_data[i],nflip[i], num_channel);\n",
    "        f_pos = np.reshape(F_pos, (img_size**2,1));\n",
    "        f_neg = np.reshape(F_neg, (img_size**2,1));\n",
    "        Gt = np.reshape((1/img_size)*np.dot(H, f_pos-f_neg), (img_size, img_size));\n",
    "        max_list.append(np.amax(Gt)-np.amin(Gt));\n",
    "        Gt = 2*(Gt-np.amin(Gt))/(np.amax(Gt)-np.amin(Gt))-1;\n",
    "        gt_list.append(Gt);\n",
    "    return gt_list, max_list\n",
    "\n",
    "def raw_ground_truth_list_index(expe_data,nflip, H, img_size, num_channel=548):\n",
    "    gt_list = [];\n",
    "    for i in range(len(nflip)):\n",
    "        F_pos , F_neg = read_mat_data_index(expe_data[i],nflip[i], num_channel);\n",
    "        f_pos = np.reshape(F_pos, (img_size**2,1));\n",
    "        f_neg = np.reshape(F_neg, (img_size**2,1));\n",
    "        Gt = np.reshape((1/img_size)*np.dot(H, f_pos-f_neg), (img_size, img_size));\n",
    "        gt_list.append(Gt);\n",
    "    return gt_list\n",
    "\n",
    "\n",
    "def ground_truth_list(expe_data,nflip, H, img_size, lambda_min = 460, lambda_max = 700):\n",
    "    gt_list = [];\n",
    "    max_list = [];\n",
    "    for i in range(len(nflip)):\n",
    "        F_pos , F_neg = read_mat_data(expe_data[i],nflip[i], lambda_min, lambda_max);\n",
    "        f_pos = np.reshape(F_pos, (img_size**2,1));\n",
    "        f_neg = np.reshape(F_neg, (img_size**2,1));\n",
    "        Gt = np.reshape((1/img_size)*np.dot(H, f_pos-f_neg), (img_size, img_size));\n",
    "        max_list.append(np.amax(Gt)-np.amin(Gt));\n",
    "        Gt = 2*(Gt-np.amin(Gt))/(np.amax(Gt)-np.amin(Gt))-1;\n",
    "        gt_list.append(Gt);\n",
    "    return gt_list, max_list\n",
    "\n",
    "def net_list(img_size, CR, Mean_had, Cov_had,net_arch, N0_list, sig, denoise, H, suffix, model_root):\n",
    "    net_type = ['c0mp', 'comp','pinv', 'free']\n",
    "    list_nets = [];\n",
    "    for N0 in N0_list:\n",
    "        recon_type = \"\";\n",
    "        if N0==0:\n",
    "            train_type = ''\n",
    "        else :\n",
    "            train_type = '_N0_{:g}_sig_{:g}'.format(N0,sig)\n",
    "            if denoise:\n",
    "                recon_type+=\"_Denoi\";\n",
    "        #- training parameters\n",
    "        title = model_root + 'NET_'+net_type[net_arch]+train_type+recon_type+suffix ;\n",
    "        if N0==0:\n",
    "            model = compNet(img_size, CR, Mean_had, Cov_had, net_arch, H)\n",
    "        else:\n",
    "            if denoise:\n",
    "                model = DenoiCompNet(img_size, CR, Mean_had, Cov_had, net_arch, N0, sig, H);\n",
    "            else:\n",
    "                model = noiCompNet(img_size, CR, Mean_had, Cov_had, net_arch, N0, sig, H);\n",
    "        model = model.to(device);\n",
    "        load_net(title,model);\n",
    "        list_nets.append(model);\n",
    "    return list_nets;\n",
    "\n",
    "def simulated_noisy_images(gt_list, max_list, K, H):\n",
    "    gt_index = max_list.index(max(max_list));\n",
    "    GT = gt_list[gt_index];\n",
    "    N = GT.shape[0];\n",
    "    H_pos = np.zeros(H.shape);\n",
    "    H_neg = np.zeros(H.shape);\n",
    "    H_pos[H>0] = N*H[H>0];\n",
    "    H_neg[H<0] = -N*H[H<0];\n",
    "    simu_list = [];\n",
    "    for i in range(len(gt_list)):\n",
    "        if i!=gt_index:\n",
    "            f_noi = simulated_measurement(GT, max_list[i]/K, H_pos, H_neg, N, H)\n",
    "        else:\n",
    "            f_noi = GT;\n",
    "        simu_list.append(f_noi)\n",
    "    return simu_list\n",
    "\n",
    "\n",
    "def simulated_measurement(GT, N0, H_pos, H_neg, N, H):\n",
    "    f = N0*np.reshape(((GT-np.amin(GT))/(np.amax(GT)-np.amin(GT))), (N**2,1));\n",
    "    m_pos = np.dot(H_pos,f);\n",
    "    m_neg = np.dot(H_neg,f);\n",
    "    m_pos += np.multiply(np.sqrt(m_pos),np.random.normal(0,1,size = m_pos.shape));\n",
    "    m_neg += np.multiply(np.sqrt(m_neg),np.random.normal(0,1,size = m_neg.shape));\n",
    "    m_noi = m_pos-m_neg;\n",
    "    f_noi = np.reshape((1/N)*np.dot(H, m_pos-m_neg), (N, N));\n",
    "    f_noi = 2*(f_noi-np.amin(f_noi))/(np.amax(f_noi)-np.amin(f_noi))-1;\n",
    "    return f_noi;\n",
    "\n",
    "\n",
    "def normalize(Img, a, b):\n",
    "    return (a-b)*(Img-np.amin(Img))/(np.amax(Img)-np.amin(Img))+b;\n",
    "\n",
    "\n",
    "\n",
    "def batch_flipud(vid):\n",
    "    outs = vid;\n",
    "    for i in range(vid.shape[1]):\n",
    "        outs[0,i,0,:,:] = np.flipud(vid[0,i,0,:,:]);\n",
    "    return outs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions to improve the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_param_reg(f,g):\n",
    "    N = f.shape[-2]*f.shape[-1];\n",
    "    f_vec = np.reshape(f[0,0,:,:], (N,1));\n",
    "    g_vec = np.reshape(g[0,0,:,:], (N,1));\n",
    "    f_mean = np.mean(f_vec);\n",
    "    g_mean = np.mean(g_vec);\n",
    "    g_norm = np.dot(np.transpose(g_vec), g_vec);\n",
    "    g_f = np.dot(np.transpose(g_vec), f_vec);\n",
    "    a = (g_f - N*f_mean*g_mean)/(g_norm-g_mean**2);\n",
    "    b = (f_mean*g_norm - g_mean*g_f)/(g_norm-g_mean**2);\n",
    "    return a*g+b;\n",
    "\n",
    "\n",
    "def single_param_reg(f,g):\n",
    "    N = f.shape[-2]*f.shape[-1];\n",
    "    f_vec = np.reshape(f[0,0,:,:], (N,1));\n",
    "    g_vec = np.reshape(g[0,0,:,:], (N,1));\n",
    "    g_norm = np.dot(np.transpose(g_vec), g_vec);\n",
    "    g_f = np.dot(np.transpose(g_vec), f_vec);\n",
    "    a = (g_f)/(g_norm);\n",
    "    return a*g;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import aslinearoperator\n",
    "import pylops\n",
    "\n",
    "#mu = 1.5\n",
    "def TV(y, H, img_size, mu = 0.15, lamda = [0.1, 0.1], niter = 20, niterinner = 10):\n",
    "    ny = img_size;\n",
    "    nx = img_size;\n",
    "    A = aslinearoperator(H);\n",
    "    H_p = pylops.LinearOperator(A)\n",
    "    Dop = \\\n",
    "        [pylops.FirstDerivative(ny * nx, dims=(ny, nx), dir=0, edge=False,\n",
    "                                kind='backward', dtype=np.float64),\n",
    "         pylops.FirstDerivative(ny * nx, dims=(ny, nx), dir=1, edge=False,\n",
    "                                kind='backward', dtype=np.float64)]\n",
    "    xinv, niter = \\\n",
    "    pylops.optimization.sparsity.SplitBregman(H_p, Dop, y.flatten(),\n",
    "                                              niter, niterinner,\n",
    "                                              mu=mu, epsRL1s=lamda,\n",
    "                                              tol=1e-4, tau=1., show=False,\n",
    "                                              **dict(iter_lim=5, damp=1e-4))\n",
    "    return xinv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bm3d import bm3d, BM3DProfile\n",
    "\n",
    "\n",
    "def diag(y):\n",
    "    n = y.shape[0];\n",
    "    D = np.zeros((n,n));\n",
    "    D[np.diag_indices(n)] = np.reshape(y,(n,));\n",
    "    return D;\n",
    "\n",
    "def Diag(A):\n",
    "    a,b = A.shape;\n",
    "    n = min(a,b);\n",
    "    d = np.reshape(A[np.diag_indices(n)], (n,1));\n",
    "    return d;\n",
    "\n",
    "\n",
    "def Denoi_stat_comp(y, Sigma, Sigma_a, H):\n",
    "    Pat = np.dot(Sigma, np.transpose(H));\n",
    "    P_k = np.linalg.inv(Sigma_a+np.dot(H,Pat));\n",
    "    P_k = np.dot(Pat, P_k);\n",
    "    x_k = np.dot(P_k,y);\n",
    "    P_k = Sigma + np.dot(P_k, np.transpose(Pat));\n",
    "    return x_k, P_k\n",
    "\n",
    "def BM3D_tikho(x_noi, P_k):\n",
    "    sigma = np.mean(Diag(P_k));\n",
    "    x_est = bm3d(x_noi, sigma);\n",
    "    return x_est;\n",
    "\n",
    "def BM3D_tikho_from_meas_expe(m, CR, img_size, H_k, Cov_had, Perm, H, g, C, s):\n",
    "    even_index = range(0,2*CR,2);\n",
    "    odd_index = range(1,2*CR,2);\n",
    "    \n",
    "    y_k_pos = m[even_index];\n",
    "    y_k_neg = m[odd_index];\n",
    "    Sigma_a = 1/g*(y_k_pos+y_k_neg)- 2*C/g +2*s**2/g**2;\n",
    "    One = np.dot(H_k, np.ones((img_size**2,)));\n",
    "    y_k = 1/g*(y_k_pos-y_k_neg);\n",
    "    alpha_est = np.amax(np.dot(np.dot(np.transpose(H_k), np.linalg.inv(np.dot(H_k, np.transpose(H_k)))),y_k))\n",
    "    Sigma_a = Sigma_a/(alpha_est**2);\n",
    "    y_k = 2/alpha_est*y_k - One;\n",
    "    Sigma_a = diag(Sigma_a);\n",
    "    \n",
    "    Sigma = img_size**2*np.dot(Perm,np.dot(Cov_had,np.transpose(Perm)));\n",
    "    Sigma_i = 1/img_size**2*np.dot(np.transpose(H),np.dot(Cov_had,H))\n",
    "    x_k_est, P_k = Denoi_stat_comp(y_k, Sigma_i, Sigma_a, H_k);\n",
    "    x_k_est = np.reshape(x_k_est, (img_size, img_size))\n",
    "    x_k_bm3d = BM3D_tikho(x_k_est, P_k);\n",
    "    return x_k_bm3d;\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "### Acquisition Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64; # Height / width dimension\n",
    "sig =0.5; # std maximum total number of photons\n",
    "K =1.6; # Normalisation constant\n",
    "C = 1070;\n",
    "s = 55;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../../data/\", # Path to SLT-10 dataset\n",
    "net_arch = 0;   # Network architecture (variants for the FCL)\n",
    "precompute_root =\"../../models/SDCAN/\"# Path to precomputed data\n",
    "precompute =  False # Tells if the precomputed data is available \n",
    "model_root = '../../models/SDCAN/'; #  Path to model saving files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expe_root = \"../../data/expe_2/\" # Path to precomputed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 ; #Number of training epochs \n",
    "batch_size = 256 ; # Size of each training batch\n",
    "reg = 1e-7; # Regularisation Parameter\n",
    "lr = 1e-3; # Learning Rate\n",
    "step_size = 10; #Scheduler Step Size\n",
    "gamma =0.5; # Scheduler Decrease Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform_file = Path(expe_root) / ('transform_{}x{}'.format(img_size, img_size)+'.mat')\n",
    "H = sio.loadmat(my_transform_file);\n",
    "H = (1/img_size)*H[\"H\"]\n",
    "#H_2 = Hadamard_Transform_Matrix(opt.img_size);\n",
    "\n",
    "\n",
    "my_average_file = Path(precompute_root) / ('Average_{}x{}'.format(img_size, img_size)+'.npy')\n",
    "my_cov_file = Path(precompute_root) / ('Cov_{}x{}'.format(img_size, img_size)+'.npy')\n",
    "\n",
    "Path(precompute_root).mkdir(parents=True, exist_ok=True)\n",
    "if not(my_average_file.is_file()) or not(my_cov_file.is_file()) or precompute:\n",
    "    print('Computing covariance and mean (overwrite previous files)')\n",
    "    Mean_had, Cov_had = Stat_had(trainloader, precompute_root)\n",
    "else:\n",
    "    print('Loading covariance and mean')\n",
    "    Mean_had = np.load(my_average_file)\n",
    "    Cov_had  = np.load(my_cov_file)\n",
    "\n",
    "\n",
    "my_average_file = Path(expe_root) / ('Average_{}x{}'.format(img_size, img_size)+'.mat')\n",
    "my_cov_file = Path(expe_root) / ('Cov_{}x{}'.format(img_size, img_size)+'.mat')\n",
    "\n",
    "print('Loading covariance and mean')\n",
    "Mean_had_1 = sio.loadmat(my_average_file)\n",
    "Cov_had_1  = sio.loadmat(my_cov_file)\n",
    "\n",
    "\n",
    "# Normalisation of imported Mean and Covariance.\n",
    "\n",
    "Mean_had_1 = Mean_had_1[\"mu\"]-np.dot(H, np.ones((img_size**2,1)));\n",
    "Mean_had_1 = np.reshape(Mean_had_1,(img_size, img_size));\n",
    "Mean_had_1 = np.amax(Mean_had)/np.amax(Mean_had_1)*Mean_had_1;\n",
    "Cov_had_1 = Cov_had_1[\"C\"];\n",
    "Cov_had_1 = np.amax(Cov_had)/np.amax(Cov_had_1)*Cov_had_1;\n",
    "     \n",
    "Var = Cov2Var(Cov_had_1)\n",
    "Perm = Permutation_Matrix(Var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed Reconstruction via CNN (CR = 7/8)\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = 512; # Number of patterns\n",
    "even_index = range(0,2*CR,2);\n",
    "uneven_index = range(1,2*CR,2);\n",
    "\n",
    "\n",
    "# Var = Cov2Var(Cov_had)\n",
    "# Perm = Permutation_Matrix(Var)\n",
    "Pmat = np.dot(Perm,H);\n",
    "H_k = Pmat[:CR,:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Relevant Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "suffix = '_N_{}_M_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\\\n",
    "        img_size, CR, num_epochs, lr, step_size,\\\n",
    "        gamma, batch_size, reg)\n",
    "\n",
    "# N0_list = [700, 150, 80, 60, 30, 10, 10];\n",
    "# N0_list_OG = [500, 150, 80, 60, 30, 10, 2];\n",
    "N0_list = [2500];\n",
    "# with HiddenPrints():\n",
    "model_list = net_list(img_size, CR, Mean_had, Cov_had,net_arch, N0_list, sig, 0, H, suffix, model_root);\n",
    "model_list_denoi = net_list(img_size, CR, Mean_had, Cov_had,net_arch, N0_list, sig, 1, H, suffix, model_root);\n",
    "model_list_no_noise = net_list(img_size, CR, Mean_had, Cov_had,net_arch, [0 for i in range(len(N0_list))], sig, 1, H, suffix,model_root);\n",
    "\n",
    "model = noiCompNet(img_size, CR, Mean_had, Cov_had, 3, 50, 0.5, H)\n",
    "root_model = '../../models/OE/NET_free_N0_2500_sig_0.5_N_64_M_512_epo_20_lr_0.001_sss_10_sdr_0.5_bs_256_reg_1e-07'\n",
    "load_net(root_model,model);\n",
    "model = model.to(device)\n",
    "\n",
    "titles = [\"GT\",  \"TV\", \"Tikhonov\",\"Noiseless Net\", \"Free Layer\", \"Tikhonov+bm3d\", \"Proposed\"]\n",
    "\n",
    "# titles = [\"GT\", \"PI\",  \"TV\", \"Tikhonov\",\"Noiseless Net\", \"Free Layer\", \"Tikhonov+bm3d\", \"Proposed\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LED Lamp - Part 3\n",
    "### Loading the Compressed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_expe = [\"noObjectD_1_0.0_variance\", \"noObjectD_1_0.3_02_variance\"]+\\\n",
    "              [\"noObjectD_1_0.3_03_variance\", \"noObjectD_1_0.3_04_variance\"]+\\\n",
    "              [\"noObjectD_1_0.3_01_variance\"]+\\\n",
    "              [\"noObjectD_1_0.3_01_variance\", \"noObjectD_1_0.6_variance\"]+\\\n",
    "              [\"noObjectD_1_1.0_variance\", \"noObjectD_1_1.3_variance\"]\n",
    "\n",
    "channel = 548;\n",
    "\n",
    "nflip = [1 for i in range(len(titles_expe))];\n",
    "expe_data = [expe_root+titles_expe[i] for i in range(len(titles_expe))];\n",
    "\n",
    "m_list = load_data_list_index(expe_data, nflip, CR, K, Perm, img_size, num_channel = channel);\n",
    "\n",
    "\n",
    "m_prim = [];\n",
    "m_prim.append(sum(m_list[:4])+m_list[6]);\n",
    "m_prim.append(sum(m_list[:2]));\n",
    "m_prim.append(m_list[0]);\n",
    "m_prim.append(m_list[6]+m_list[8]);\n",
    "m_prim = m_prim+m_list[7:];\n",
    "m_list = m_prim;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ground Truth\n",
    "We normalize the incoming data, so that it has the right functioning range for neural networks to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT=raw_ground_truth_list_index(expe_data, nflip, H, img_size, num_channel = channel);\n",
    "# Good values 450 - 530 -  548 - 600\n",
    "GT_prim = [];\n",
    "GT_prim.append(sum(GT[:4])+GT[6]);\n",
    "GT_prim.append(sum(GT[:2]));\n",
    "GT_prim.append(GT[0]);\n",
    "GT_prim.append(GT[6]+GT[8]);\n",
    "GT_prim = GT_prim+GT[7:];\n",
    "GT = GT_prim;\n",
    "max_list = [np.amax(GT[i])-np.amin(GT[i]) for i in range(len(GT))];\n",
    "#    GT = [((GT[i]-np.amin(GT[i]))/max_list[i]+1)/2 for i in range(len(GT))];\n",
    "GT = [((GT[i]-np.amin(GT[i]))/max_list[i])*2-1 for i in range(len(GT))];\n",
    "max_list = [max_list[i]/K for i in range(len(max_list))];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the results\n",
    "Once all the networks have been loaded, we appy those networks on the loaded Compressed Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "\n",
    "from time import perf_counter\n",
    "title_lists = [];\n",
    "Additional_info = [[\"N0 = {}\".format(round(max_list[i])) if j==0 else \"\" for j in range(len(titles))] for i in range (len(max_list))]\n",
    "Ground_truth = torch.Tensor(GT[0]).view(1,1,1,img_size, img_size).repeat(1,len(titles),1,1,1);\n",
    "outputs = [];\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(GT)):\n",
    "        list_outs = [];\n",
    "        x_Pinv = model_list[0].forward_N0_Pinv(1/K*m_list[i]*4, 1, 1, img_size, img_size);\n",
    "        x_Stat_comp = model_list[0].forward_N0_maptoimage(1/K*m_list[i]*4, 1, 1, img_size, img_size);\n",
    "        x_Denoi_Stat_comp = model_list_denoi[0].forward_N0_maptoimage_expe(m_list[i]*4, 1, 1, img_size, img_size, C, s, K);\n",
    "\n",
    "        t1_start = perf_counter() \n",
    "        x_SDCAN = model_list_no_noise[0].forward_postprocess(x_Stat_comp, 1,1, img_size, img_size);\n",
    "        t1_stop = perf_counter() \n",
    "        print(\"CNN time = {}s\".format(t1_stop-t1_start))\n",
    "        \n",
    "        x_SDCAN_denoi = model_list_denoi[0].forward_N0_reconstruct_expe(m_list[i]*4,1,1, img_size, img_size, C, s, K);\n",
    "        x_free = model.forward_N0_reconstruct_expe(m_list[i]*4,1,1, img_size, img_size, C, s, K);\n",
    "        \n",
    "        \n",
    "#         print(x_bm3d.shape)\n",
    "        \n",
    "        m = torch2numpy(m_list[i][0,0,even_index] - m_list[i][0,0,uneven_index]);\n",
    "        alpha_est = np.amax(np.dot(np.dot(np.transpose(H_k), np.linalg.inv(np.dot(H_k, np.transpose(H_k)))),m))\n",
    "        x_tv = 2*TV(m/alpha_est, H_k, img_size)-1;\n",
    "#         x_bm3d = bm3d(torch2numpy(x_Denoi_Stat_comp[0,0,:,:]),1/alpha_est**2*(1/K*np.mean(torch2numpy(m_list[i][0,0,even_index]+m_list[i][0,0,uneven_index]))- 2*C/K +2*s**2/K**2))\n",
    "        \n",
    "        t1_start = perf_counter() \n",
    "        x_bm3d = bm3d(torch2numpy(x_Denoi_Stat_comp[0,0,:,:]),0.3)\n",
    "        t1_stop = perf_counter() \n",
    "        print(\"BM3D time = {}s\".format(t1_stop-t1_start))\n",
    "#         x_bm3d = BM3D_tikho_from_meas_expe(torch2numpy(m_list[i][0,0,:]), CR, img_size, H_k, Cov_had, Perm, H, K, C, s)\n",
    "\n",
    "#         print(np.amax(x_tv))\n",
    "#         print(np.amin(x_tv))\n",
    "\n",
    "        x_bm3d = torch.Tensor(x_bm3d).to(device);\n",
    "        x_bm3d = x_bm3d.view(1,1, img_size, img_size);\n",
    "        x_tv = torch.Tensor(x_tv).to(device);\n",
    "        x_tv = x_tv.view(1,1, img_size, img_size);\n",
    "        \n",
    "        gt = torch.Tensor(GT[i]).to(device);\n",
    "        gt = gt.view(1,1, img_size, img_size);\n",
    "        list_outs.append(gt);\n",
    "#         list_outs.append(x_Pinv);\n",
    "        list_outs.append(x_tv);\n",
    "        list_outs.append(x_Denoi_Stat_comp);\n",
    "        list_outs.append(x_SDCAN);\n",
    "        list_outs.append(x_free);\n",
    "        list_outs.append(x_bm3d);\n",
    "        list_outs.append(x_SDCAN_denoi);\n",
    "        output = torch.stack(list_outs, axis = 1);\n",
    "\n",
    "        psnr = batch_psnr_vid(Ground_truth, output);\n",
    "#         ssim = batch_ssim_vid(Ground_truth, output);\n",
    "        outputs.append(torch2numpy(output));\n",
    "#         title_lists.append([\"{} {},\\n PSNR = {},\\n SSIM = {}\".format(titles[j],Additional_info[i][j], round(psnr[j],2),round( ssim[j],2)) for j in range(len(titles))]);\n",
    "        title_lists.append([\"{} {},\\n PSNR = {}\".format(titles[j],Additional_info[i][j], round(psnr[j],2)) for j in range(len(titles))]);\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = outputs;\n",
    "t1 = title_lists;\n",
    "nb_disp_frames = 7;\n",
    "#compare_video_frames(outputs, nb_disp_frames, title_lists);\n",
    "outputs_0 = outputs[:1];\n",
    "outputs_1 = outputs[1:4];\n",
    "outputs_2 = outputs[4:];\n",
    "title_lists_0 = title_lists[:1];\n",
    "title_lists_1 = title_lists[1:4];\n",
    "title_lists_2 = title_lists[4:];\n",
    "\n",
    "compare_video_frames(outputs_0, nb_disp_frames, title_lists_0);\n",
    "compare_video_frames(outputs_1, nb_disp_frames, title_lists_1);\n",
    "compare_video_frames(outputs_2, nb_disp_frames, title_lists_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL-10 Cat \n",
    "### Loading the Compressed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_expe = [\"stl10_05_1.5_0.0_0{}_variance\".format(i) for i in range(1,7)]+\\\n",
    "              [\"stl10_05_1_0.3_variance\", \"stl10_05_1_0.6_variance\"]\n",
    "\n",
    "expe_data = [expe_root+titles_expe[i] for i in range(len(titles_expe))];\n",
    "nflip = [1.5 for i in range(len(titles_expe))];\n",
    "nflip[-2:] = [1 for i in range(len(nflip[-2:]))]\n",
    "channel = 581;\n",
    "m_list = load_data_list_index(expe_data, nflip, CR, K, Perm, img_size, num_channel = channel);\n",
    "\n",
    "\n",
    "m_prim = [];\n",
    "m_prim = [];\n",
    "m_prim.append(sum(m_list[:7]));\n",
    "m_prim.append(m_list[0]+m_list[1]);\n",
    "m_prim.append(m_list[2]);\n",
    "m_prim = m_prim+m_list[-2:];\n",
    "m_list = m_prim;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ground Truth\n",
    "We normalize the incoming data, so that it has the right functioning range for neural networks to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT=raw_ground_truth_list_index(expe_data, nflip, H, img_size, num_channel = channel);\n",
    "# Good values 450 - 530 -  548 - 600\n",
    "GT_prim = [];\n",
    "GT_prim.append(sum(GT[:7]));\n",
    "GT_prim.append(GT[0]+GT[1]);\n",
    "GT_prim.append(GT[2]);\n",
    "GT_prim = GT_prim+GT[-2:];\n",
    "GT = GT_prim;\n",
    "max_list = [np.amax(GT[i])-np.amin(GT[i]) for i in range(len(GT))];\n",
    "#    GT = [((GT[i]-np.amin(GT[i]))/max_list[i]+1)/2 for i in range(len(GT))];\n",
    "GT = [((GT[i]-np.amin(GT[i]))/max_list[i])*2-1 for i in range(len(GT))];\n",
    "max_list = [max_list[i]/K for i in range(len(max_list))];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the results\n",
    "Once all the networks have been loaded, we appy those networks on the loaded Compressed Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "\n",
    "\n",
    "title_lists = [];\n",
    "Additional_info = [[\"N0 = {}\".format(round(max_list[i])) if j==0 else \"\" for j in range(len(titles))] for i in range (len(max_list))]\n",
    "Ground_truth = torch.Tensor(GT[0]).view(1,1,1,img_size, img_size).repeat(1,len(titles),1,1,1);\n",
    "outputs = [];\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(GT)):\n",
    "        list_outs = [];\n",
    "        x_Pinv = model_list[0].forward_N0_Pinv(1/K*m_list[i]*4, 1, 1, img_size, img_size);\n",
    "        x_Stat_comp = model_list[0].forward_N0_maptoimage(1/K*m_list[i]*4, 1, 1, img_size, img_size);\n",
    "        x_Denoi_Stat_comp = model_list_denoi[0].forward_N0_maptoimage_expe(m_list[i]*4, 1, 1, img_size, img_size, C, s, K);\n",
    "        x_SDCAN = model_list_no_noise[0].forward_postprocess(x_Stat_comp, 1,1, img_size, img_size);\n",
    "        x_SDCAN_denoi = model_list_denoi[0].forward_N0_reconstruct_expe(m_list[i]*4,1,1, img_size, img_size, C, s, K);\n",
    "        x_free = model.forward_N0_reconstruct_expe(m_list[i]*4,1,1, img_size, img_size, C, s, K);\n",
    "\n",
    "            \n",
    "        m = torch2numpy(m_list[i][0,0,even_index] - m_list[i][0,0,uneven_index]);\n",
    "        alpha_est = np.amax(np.dot(np.dot(np.transpose(H_k), np.linalg.inv(np.dot(H_k, np.transpose(H_k)))),m))\n",
    "        x_tv = 2*TV(m/alpha_est, H_k, img_size)-1;\n",
    "#         x_bm3d = bm3d(torch2numpy(x_Denoi_Stat_comp[0,0,:,:]),1/alpha_est**2*(1/K*np.mean(torch2numpy(m_list[i][0,0,even_index]+m_list[i][0,0,uneven_index]))- 2*C/K +2*s**2/K**2))\n",
    "        x_bm3d = bm3d(torch2numpy(x_Denoi_Stat_comp[0,0,:,:]),0.3)\n",
    "\n",
    "#         print(np.amax(x_tv))\n",
    "#         print(np.amin(x_tv))\n",
    "\n",
    "        x_bm3d = torch.Tensor(x_bm3d).to(device);\n",
    "        x_bm3d = x_bm3d.view(1,1, img_size, img_size);\n",
    "        x_tv = torch.Tensor(x_tv).to(device);\n",
    "        x_tv = x_tv.view(1,1, img_size, img_size);\n",
    "        \n",
    "        gt = torch.Tensor(GT[i]).to(device);\n",
    "        gt = gt.view(1,1, img_size, img_size);\n",
    "        list_outs.append(gt);\n",
    "#         list_outs.append(x_Pinv);\n",
    "        list_outs.append(x_tv);\n",
    "        list_outs.append(x_Denoi_Stat_comp);\n",
    "        list_outs.append(x_SDCAN);\n",
    "        list_outs.append(x_free);\n",
    "        list_outs.append(x_bm3d);\n",
    "        list_outs.append(x_SDCAN_denoi);\n",
    "        output = torch.stack(list_outs, axis = 1);\n",
    "\n",
    "        psnr = batch_psnr_vid(Ground_truth, output);\n",
    "#         ssim = batch_ssim_vid(Ground_truth, output);\n",
    "        output = torch2numpy(output);\n",
    "        output = batch_flipud(output);\n",
    "        outputs.append(output);\n",
    "#         title_lists.append([\"{} {},\\n PSNR = {},\\n SSIM = {}\".format(titles[j],Additional_info[i][j], round(psnr[j],2),round( ssim[j],2)) for j in range(len(titles))]);\n",
    "        title_lists.append([\"{} {},\\n PSNR = {}\".format(titles[j],Additional_info[i][j], round(psnr[j],2)) for j in range(len(titles))]);\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2 = outputs;\n",
    "t2 = title_lists;\n",
    "nb_disp_frames = 7;\n",
    "#compare_video_frames(outputs, nb_disp_frames, title_lists);\n",
    "outputs_0 = outputs[:1];\n",
    "outputs_1 = outputs[1:4];\n",
    "outputs_2 = outputs[4:];\n",
    "title_lists_0 = title_lists[:1];\n",
    "title_lists_1 = title_lists[1:4];\n",
    "title_lists_2 = title_lists[4:];\n",
    "\n",
    "compare_video_frames(outputs_0, nb_disp_frames, title_lists_0);\n",
    "compare_video_frames(outputs_1, nb_disp_frames, title_lists_1);\n",
    "compare_video_frames(outputs_2, nb_disp_frames, title_lists_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed Reconstruction via CNN (CR = 3/4)\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100 ; #Number of training epochs \n",
    "num_epochs = 20 ; #Number of training epochs \n",
    "batch_size = 256 ; # Size of each training batch\n",
    "reg = 1e-7; # Regularisation Parameter\n",
    "lr = 1e-3; # Learning Rate\n",
    "step_size = 10; #Scheduler Step Size\n",
    "gamma =0.5; # Scheduler Decrease Rate\n",
    "\n",
    "\n",
    "CR = 1024; # Number of patterns\n",
    "# CR = 2048; # Number of patterns\n",
    "\n",
    "even_index = range(0,2*CR,2);\n",
    "uneven_index = range(1,2*CR,2);\n",
    "\n",
    "\n",
    "# Var = Cov2Var(Cov_had)\n",
    "# Perm = Permutation_Matrix(Var)\n",
    "Pmat = np.dot(Perm,H);\n",
    "H_k = Pmat[:CR,:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Relevant Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suffix = '_N_{}_M_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\\\n",
    "        img_size, CR, num_epochs, lr, step_size,\\\n",
    "        gamma, batch_size, reg)\n",
    "\n",
    "# N0_list = [700, 150, 80, 60, 30, 10, 10];\n",
    "# N0_list_OG = [500, 150, 80, 60, 30, 10, 2];\n",
    "N0_list = [2500];\n",
    "# N0_list = [50];\n",
    "\n",
    "# with HiddenPrints():\n",
    "model_list = net_list(img_size, CR, Mean_had, Cov_had,net_arch, N0_list, sig, 0, H, suffix, model_root);\n",
    "model_list_denoi = net_list(img_size, CR, Mean_had, Cov_had,net_arch, N0_list, sig, 1, H, suffix, model_root);\n",
    "model_list_no_noise = net_list(img_size, CR, Mean_had, Cov_had,net_arch, [0 for i in range(len(N0_list))], sig, 1, H, suffix,model_root);\n",
    "\n",
    "model = noiCompNet(img_size, CR, Mean_had, Cov_had, 3, 50, 0.5, H)\n",
    "root_model = '../../models/OE/NET_free_N0_2500_sig_0.5_N_64_M_1024_epo_20_lr_0.001_sss_10_sdr_0.5_bs_256_reg_1e-07'\n",
    "load_net(root_model,model);\n",
    "model = model.to(device)\n",
    "titles = [\"GT\",  \"TV\", \"Tikhonov\",\"Noiseless Net\", \"Free Layer\", \"Tikhonov+bm3d\", \"Proposed\"]\n",
    "\n",
    "# titles = [\"GT\", \"PI\",  \"TV\", \"Tikhonov\",\"Noiseless Net\", \"Free Layer\", \"Tikhonov+bm3d\", \"Proposed\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siemens Star\n",
    "### Loading the Compressed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_expe = [\"starSectorD_2_0.0_01_variance\", \"starSectorD_2_0.0_02_variance\"]+\\\n",
    "              [\"starSectorD_2_0.0_03_variance\", \"starSectorD_2_0.0_04_variance\"]+\\\n",
    "              [\"starSectorD_2_0.0_05_variance\", \"starSectorD_2_0.0_06_variance\"]+\\\n",
    "              [\"starSectorD_2_0.0_07_variance\", \"starSectorD_2_0.0_08_variance\"]+\\\n",
    "              [\"starSectorD_2_0.0_09_variance\", \"starSectorD_2_0.0_variance\"]+\\\n",
    "              [\"starSectorD_2_0.3_variance\", \"starSectorD_2_0.6_variance\"]+\\\n",
    "              [\"starSectorD_2_1.0_variance\", \"starSectorD_2_1.3_variance\"]\n",
    "\n",
    "channel = 510;\n",
    "\n",
    "nflip = [2 for i in range(len(titles_expe))];\n",
    "expe_data = [expe_root+titles_expe[i] for i in range(len(titles_expe))];\n",
    "\n",
    "m_list = load_data_list_index(expe_data, nflip, CR, K, Perm, img_size, num_channel = channel);\n",
    "    \n",
    "m_prim = [];\n",
    "m_prim.append(sum(m_list[:10]));\n",
    "m_prim.append(sum(m_list[7:9]));\n",
    "m_prim = m_prim+m_list[9:];\n",
    "m_list = m_prim;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ground Truth\n",
    "We normalize the incoming data, so that it has the right functioning range for neural networks to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GT=raw_ground_truth_list_index(expe_data, nflip, H, img_size, num_channel = channel);\n",
    "# Good values 450 - 530 -  548 - 600 -510\n",
    "\n",
    "GT_prim = [];\n",
    "GT_prim.append(sum(GT[:10]));\n",
    "GT_prim.append(sum(GT[7:9]));\n",
    "GT_prim = GT_prim+GT[9:];\n",
    "GT = GT_prim;\n",
    "max_list = [np.amax(GT[i])-np.amin(GT[i]) for i in range(len(GT))];\n",
    "#    GT = [((GT[i]-np.amin(GT[i]))/max_list[i]+1)/2 for i in range(len(GT))];\n",
    "GT = [((GT[i]-np.amin(GT[i]))/max_list[i])*2-1 for i in range(len(GT))];\n",
    "max_list = [max_list[i]/K for i in range(len(max_list))];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the results\n",
    "Once all the networks have been loaded, we appy those networks on the loaded Compressed Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "\n",
    "\n",
    "title_lists = [];\n",
    "Additional_info = [[\"N0 = {}\".format(round(max_list[i])) if j==0 else \"\" for j in range(len(titles))] for i in range (len(max_list))]\n",
    "Ground_truth = torch.Tensor(GT[0]).view(1,1,1,img_size, img_size).repeat(1,len(titles),1,1,1);\n",
    "outputs = [];\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(GT)):\n",
    "        list_outs = [];\n",
    "        x_Pinv = model_list[0].forward_N0_Pinv(1/K*m_list[i]*4, 1, 1, img_size, img_size);\n",
    "        x_Stat_comp = model_list[0].forward_N0_maptoimage(1/K*m_list[i]*4, 1, 1, img_size, img_size);\n",
    "        x_Denoi_Stat_comp = model_list_denoi[0].forward_N0_maptoimage_expe(m_list[i]*4, 1, 1, img_size, img_size, C, s, K);\n",
    "        x_SDCAN = model_list_no_noise[0].forward_postprocess(x_Stat_comp, 1,1, img_size, img_size);\n",
    "        x_SDCAN_denoi = model_list_denoi[0].forward_N0_reconstruct_expe(m_list[i]*4,1,1, img_size, img_size, C, s, K);\n",
    "        x_free = model.forward_N0_reconstruct_expe(m_list[i]*4,1,1, img_size, img_size, C, s, K);\n",
    "\n",
    "        \n",
    "        m = torch2numpy(m_list[i][0,0,even_index] - m_list[i][0,0,uneven_index]);\n",
    "        alpha_est = np.amax(np.dot(np.dot(np.transpose(H_k), np.linalg.inv(np.dot(H_k, np.transpose(H_k)))),m))\n",
    "        x_tv = 2*TV(m/alpha_est, H_k, img_size)-1;\n",
    "#         x_bm3d = bm3d(torch2numpy(x_Denoi_Stat_comp[0,0,:,:]),1/alpha_est**2*(1/K*np.mean(torch2numpy(m_list[i][0,0,even_index]+m_list[i][0,0,uneven_index]))- 2*C/K +2*s**2/K**2))\n",
    "\n",
    "        x_bm3d = bm3d(torch2numpy(x_Denoi_Stat_comp[0,0,:,:]),0.3)\n",
    "\n",
    "        \n",
    "#         print(np.amax(x_tv))\n",
    "#         print(np.amin(x_tv))\n",
    "\n",
    "        x_bm3d = torch.Tensor(x_bm3d).to(device);\n",
    "        x_bm3d = x_bm3d.view(1,1, img_size, img_size);\n",
    "        x_tv = torch.Tensor(x_tv).to(device);\n",
    "        x_tv = x_tv.view(1,1, img_size, img_size);\n",
    "        \n",
    "        gt = torch.Tensor(GT[i]).to(device);\n",
    "        gt = gt.view(1,1, img_size, img_size);\n",
    "        list_outs.append(gt);\n",
    "#         list_outs.append(x_Pinv);\n",
    "        list_outs.append(x_tv);\n",
    "        list_outs.append(x_Denoi_Stat_comp);\n",
    "        list_outs.append(x_SDCAN);\n",
    "        list_outs.append(x_free);\n",
    "        list_outs.append(x_bm3d);\n",
    "        list_outs.append(x_SDCAN_denoi);\n",
    "        output = torch.stack(list_outs, axis = 1);\n",
    "\n",
    "        psnr = batch_psnr_vid(Ground_truth, output);\n",
    "#         ssim = batch_ssim_vid(Ground_truth, output);\n",
    "        outputs.append(torch2numpy(output));\n",
    "#         title_lists.append([\"{} {},\\n PSNR = {},\\n SSIM = {}\".format(titles[j],Additional_info[i][j], round(psnr[j],2),round( ssim[j],2)) for j in range(len(titles))]);\n",
    "        title_lists.append([\"{} {},\\n PSNR = {}\".format(titles[j],Additional_info[i][j], round(psnr[j],2)) for j in range(len(titles))]);\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3 = outputs;\n",
    "t3 = title_lists;\n",
    "nb_disp_frames = 7;\n",
    "#compare_video_frames(outputs, nb_disp_frames, title_lists);\n",
    "outputs_0 = outputs[:1];\n",
    "outputs_1 = outputs[1:4];\n",
    "outputs_2 = outputs[4:];\n",
    "title_lists_0 = title_lists[:1];\n",
    "title_lists_1 = title_lists[1:4];\n",
    "title_lists_2 = title_lists[4:];\n",
    "\n",
    "compare_video_frames(outputs_0, nb_disp_frames, title_lists_0);\n",
    "compare_video_frames(outputs_1, nb_disp_frames, title_lists_1);\n",
    "compare_video_frames(outputs_2, nb_disp_frames, title_lists_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lamp = np.concatenate((np.reshape(o1[0][0,0,0,:,:],(1,1,1,img_size, img_size)), o1[-1]), axis = 1)\n",
    "title_lamp = [t1[0][0][:-11] + \"(a)\"]+t1[-1]\n",
    "\n",
    "out_cat = np.concatenate((np.reshape(o2[0][0,0,0,:,:],(1,1,1,img_size, img_size)), o2[-1]), axis = 1)\n",
    "title_cat = [t2[0][0][:-11] + \"(b)\"]+t2[-1]\n",
    "\n",
    "out_star = np.concatenate((np.reshape(o3[0][0,0,0,:,:],(1,1,1,img_size, img_size)), o3[-4]), axis = 1)\n",
    "title_star = [t3[0][0][:-11] + \"(c)\"]+t3[-4]\n",
    "\n",
    "outputs = [out_lamp, out_cat, out_star]\n",
    "title_lists = [title_lamp, title_cat, title_star]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "nb_disp_frames = 8\n",
    "compare_video_frames(outputs, nb_disp_frames, title_lists, savefig = '../../img/tci/results_expe.pdf', fontsize = 11.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(liste):\n",
    "    x = len(liste);\n",
    "    y = len(liste[0]);\n",
    "    out = [];\n",
    "    for i in range(y):\n",
    "        out_i = [];\n",
    "        for j in range(x):\n",
    "            out_i.append(liste[j][i]);\n",
    "        out.append(out_i);\n",
    "    return out;\n",
    "\n",
    "def transpose_vid(liste):\n",
    "    x = len(liste);\n",
    "    y = liste[0].shape[1];\n",
    "    n = liste[0].shape[-1]\n",
    "    out = [];\n",
    "    for i in range(y):\n",
    "        out_i = np.zeros((1,x,1,n,n));\n",
    "        for j in range(x):\n",
    "            out_i[0,j,0,:,:] = liste[j][0,i,0,:,:];\n",
    "        out.append(out_i);\n",
    "    return out;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_video_frames(transpose_vid(outputs), 3, transpose(title_lists), aspect = (9,30), savefig = '../../img/tci/results_expe.pdf', fontsize = 11.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "nb_disp_frames = 4\n",
    "\n",
    "title_lists[0][1] = \"Noisy \"+ title_lists[0][1]\n",
    "title_lists[2][1] = \"Noisy \"+ title_lists[2][1]\n",
    "title_lists[1][1] = \"Noisy \"+ title_lists[1][1]\n",
    "compare_video_frames([outputs[0][:,:4,:,:,:]], nb_disp_frames, [title_lists[0][:4]], savefig = '../../img/tci/results_expe_1.pdf', fontsize = 11.4)\n",
    "compare_video_frames([outputs[0][:,4:,:,:,:]], nb_disp_frames, [title_lists[0][4:]], savefig = '../../img/tci/results_expe_2.pdf', fontsize = 11.4)\n",
    "\n",
    "compare_video_frames([outputs[1][:,:4,:,:,:]], nb_disp_frames, [title_lists[1][:4]], savefig = '../../img/tci/results_expe_3.pdf', fontsize = 11.4)\n",
    "compare_video_frames([outputs[1][:,4:,:,:,:]], nb_disp_frames, [title_lists[1][4:]], savefig = '../../img/tci/results_expe_4.pdf', fontsize = 11.4)\n",
    "\n",
    "compare_video_frames([outputs[2][:,:4,:,:,:]], nb_disp_frames, [title_lists[2][:4]], savefig = '../../img/tci/results_expe_5.pdf', fontsize = 11.4)\n",
    "compare_video_frames([outputs[2][:,4:,:,:,:]], nb_disp_frames, [title_lists[2][4:]], savefig = '../../img/tci/results_expe_6.pdf', fontsize = 11.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
