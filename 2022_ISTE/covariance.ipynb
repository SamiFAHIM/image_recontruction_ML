{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequency-ordered (Walsh) Hadamard transform\n",
    "\n",
    "https://ieeexplore.ieee.org/document/1162377\n",
    "\n",
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import imageio\n",
    "import cv2\n",
    "import fht\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wget\n",
    "import zipfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import important functions from spyrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyrit.learning.model_Had_DCAN import *# compNet, Stat_had, Weight_Decay_Loss\n",
    "from spyrit.learning.nets import *\n",
    "from spyrit.misc.disp import *\n",
    "from spyrit.misc.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "#- Acquisition\n",
    "img_size = 64 # image size\n",
    "batch_size = 1024\n",
    "M = 333       # number of neasurements\n",
    "\n",
    "#- Model and data paths\n",
    "model_root = Path('./models/')\n",
    "stats_root = Path('./stats_ordered2/')\n",
    "data_root = Path('./data/')\n",
    "#- Save plot using type 1 font\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "print(model_root)\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load STL-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(7)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.functional.to_grayscale,\n",
    "     transforms.Resize((img_size, img_size)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "\n",
    "trainset = \\\n",
    "    torchvision.datasets.STL10(root=data_root, split='train+unlabeled',download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "testset = \\\n",
    "    torchvision.datasets.STL10(root=data_root, split='test',download=True, transform=transform)\n",
    "testloader =  torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "dataloaders = {'train':trainloader, 'val':testloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walsh ordered in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_walsh_ordered(dataloader, H1d, tot_num):\n",
    "    #-- Accumulate over all images in dataset\n",
    "    n = 0\n",
    "    Abs_had = np.zeros((H1d.shape[1],H1d.shape[1]))\n",
    "    for inputs,_ in dataloader:\n",
    "        inputs = inputs.cpu().detach().numpy();\n",
    "        for i in range(inputs.shape[0]):\n",
    "            img = inputs[i,0,:,:];\n",
    "            h_img = walsh_ordered2(img,H1d);\n",
    "            Abs_had += abs(h_img);\n",
    "            n = n+1\n",
    "        print(f'Abs:  {n} / (less than) {tot_num} images', end='\\r')\n",
    "    print('', end='\\n')\n",
    "    \n",
    "    #-- Normalize\n",
    "    Abs_had = Abs_had/n;\n",
    "    \n",
    "    return Abs_had\n",
    "\n",
    "\n",
    "def stat_walsh_ordered(dataloader, root):\n",
    "    \"\"\" \n",
    "        Computes Mean Hadamard Image over the whole dataset + \n",
    "        Covariance Matrix Amongst the coefficients\n",
    "    \"\"\"\n",
    "    inputs, classes = next(iter(dataloader))\n",
    "    inputs = inputs.cpu().detach().numpy();\n",
    "    (batch_size, channels, nx, ny) = inputs.shape;\n",
    "    tot_num = len(dataloader)*batch_size;\n",
    "    \n",
    "    H1d = wh.walsh_ordered(nx)\n",
    "    \n",
    "     # Abs matrix\n",
    "    Mean_had = abs_walsh_ordered(dataloader, H1d, tot_num)\n",
    "    print(\"Saving abs\")\n",
    "    np.save(root / Path('Abs_{}x{}'.format(nx,ny)+'.npy'), Mean_had)\n",
    "\n",
    "    # Mean matrix\n",
    "    #-- Accumulate over all images in dataset\n",
    "    n = 0\n",
    "    Mean_had = np.zeros((nx, ny));\n",
    "    for inputs,_ in dataloader:\n",
    "        inputs = inputs.cpu().detach().numpy();\n",
    "        for i in range(inputs.shape[0]):\n",
    "            img = inputs[i,0,:,:];\n",
    "            h_img = walsh_ordered2(img,H1d);\n",
    "            Mean_had += h_img;\n",
    "            n = n+1\n",
    "        print(f'Mean:  {n} / (less than) {tot_num} images', end='\\r')\n",
    "    print('', end='\\n')\n",
    "    \n",
    "    #-- Normalize & save\n",
    "    Mean_had = Mean_had/n;\n",
    "    print(\"Saving mean\")\n",
    "    np.save(root / Path('Mean_{}x{}'.format(nx,ny)+'.npy'), Mean_had)\n",
    "    \n",
    "    # Covariance matrix    \n",
    "    n = 0\n",
    "    Cov_had = np.zeros((nx*ny, nx*ny));\n",
    "    for inputs,_ in dataloader:\n",
    "        inputs = inputs.cpu().detach().numpy();\n",
    "        for i in range(inputs.shape[0]):\n",
    "            img = inputs[i,0,:,:];\n",
    "            h_img = walsh_ordered2(img, H1d);\n",
    "            Norm_Variable = np.reshape(h_img-Mean_had, (nx*ny,1));\n",
    "            Cov_had += Norm_Variable*np.transpose(Norm_Variable);\n",
    "            n = n+1\n",
    "        print(f'Covariance:  {n} / (less than) {tot_num} images', end='\\r')     \n",
    "    print()\n",
    "    \n",
    "    #-- Normalize & save\n",
    "    Cov_had = Cov_had/(n-1);  \n",
    "    np.save(root / Path('Cov_{}x{}'.format(nx,ny)+'.npy'), Cov_had)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abs:  105000 / (less than) 105472 images\n",
      "Saving abs\n",
      "Mean:  105000 / (less than) 105472 images\n",
      "Saving mean\n",
      "Covariance:  105000 / (less than) 105472 images\n"
     ]
    }
   ],
   "source": [
    "path = Path('./stats_ordered')\n",
    "\n",
    "#if not path.exists():\n",
    "#    path.mkdir()\n",
    "        \n",
    "stat_walsh_ordered(trainloader, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Cov and Mean') \n",
    "Cov_had = np.load(stats_root / Path(\"Cov_{}x{}.npy\".format(img_size, img_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1d = wh.walsh_ordered(64)\n",
    "H1d.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyrit-env",
   "language": "python",
   "name": "spyrit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
