{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openspyrit/spyrit-examples/blob/master/tutorial/tuto_train_split_meas_colab.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial to train a reconstruction network "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial to train a reconstruction network for 2D single-pixel imaging on stl10, for split measurements.\n",
    "\n",
    "Training is performed by a call to *train.py*. Several parameters allow to modify acquisition, network and training (network architecture), optimisation and the use of tensorboard. \n",
    "\n",
    "Currently you can train the following networks by modifying the network architecture variable *arch*: \n",
    "\n",
    "- 'dc-net': Denoised Completion Network (DCNet). \n",
    "- 'pinv-net': Pseudo Inverse Network (PinvNet).\n",
    "- 'upgd': Unrolled proximal gradient descent (UPGD). \n",
    "\n",
    "and the denoising variable *denoi*: \n",
    "- 'cnn': CNN no batch normalization\n",
    "- 'cnnbn': CNN with batch normalization\n",
    "- 'unet': UNet (0.5 M trainable parameters) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On colab, choose GPU at *Runtime/Change runtime type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, mount google drive to import modules spyrit modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_colab = True\n",
    "if (mode_colab is True):\n",
    "    # Connect to googledrive\n",
    "    #if 'google.colab' in str(get_ipython()):\n",
    "    # Mount google drive to access files via colab\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    %cd /content/gdrive/MyDrive/\n",
    "\n",
    "    # For the profiler\n",
    "    !pip install -U tensorboard-plugin-profile\n",
    "\n",
    "    # Load the TensorBoard notebook extension\n",
    "    %load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Spyrit package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone and install spyrit and spyrit-examples respositories if not installed. Temporally, checkout to current development branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode_colab is True):\n",
    "    # Clone and install\n",
    "    !git clone https://github.com/openspyrit/spyrit.git\n",
    "    %cd spyrit\n",
    "    !pip install -e .\n",
    "\n",
    "    # Checkout to ongoing branch\n",
    "    !git fetch --all\n",
    "\n",
    "    # Add paths for modules\n",
    "    import sys\n",
    "    sys.path.append('./spyrit/core')\n",
    "    sys.path.append('./spyrit/misc')\n",
    "    sys.path.append('./spyrit/tutorial')\n",
    "    %cd ..\n",
    "\n",
    "    # Clone Spyrit-examples and checkout to branch tutorials\n",
    "    !git clone https://github.com/openspyrit/spyrit-examples.git\n",
    "    %cd spyrit-examples\n",
    "    !git checkout train_upgd\n",
    "    %cd tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download covariance matrix. Alternatively install *openspyrit/spas* package:\n",
    "```\n",
    "    spyrit\n",
    "    ├───stat\n",
    "    │   ├───Average_64x64.npy\n",
    "    │   ├───Cov_64x64.npy\n",
    "    ├───spirit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_cov = True\n",
    "if (download_cov is True):\n",
    "    if 'stat' not in os.listdir():\n",
    "        !pip install girder-client\n",
    "        import girder_client\n",
    "\n",
    "        # api Rest url of the warehouse\n",
    "        url='https://pilot-warehouse.creatis.insa-lyon.fr/api/v1'\n",
    "        \n",
    "        # Generate the warehouse client\n",
    "        gc = girder_client.GirderClient(apiUrl=url)\n",
    "\n",
    "        #%% Download the covariance matrix and mean image\n",
    "        data_folder = './stat/'\n",
    "        dataId_list = [\n",
    "                '63935b624d15dd536f0484a5', # for reconstruction (imageNet, 64)\n",
    "                '63935a224d15dd536f048496', # for reconstruction (imageNet, 64)\n",
    "                ]\n",
    "        for dataId in dataId_list:\n",
    "            myfile = gc.getFile(dataId)\n",
    "            gc.downloadFile(dataId, data_folder + myfile['name'])\n",
    "\n",
    "        print(f'Created {data_folder}') \n",
    "        !ls $data_folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose the following parameters:\n",
    "\n",
    "- Measurement operators:\n",
    "    - `meas`: Measurement type: 'hadam-split', 'hadam-pos'\n",
    "    - `noise` = 'Noise types: 'poisson', 'gauss-approx', 'no-noise'\n",
    "    - `prep` = Preprocessing types: 'dir-poisson', 'split-poisson'\n",
    "\n",
    "- Acquisition:\n",
    "    - `img_size`: Height / width dimension, default=64\n",
    "    - `M`: Number of undersampling patterns, default=512\n",
    "    - `subs`: Among 'var','rect', default=\"var\"\n",
    "    \n",
    "- Network and training:\n",
    "    - `data`: stl10 or imagenet, default=\"stl10\"\n",
    "    - `model_root`: Path to model saving files, default='./model/'\n",
    "    - `data_root`: Path to the dataset, default=\"./data/\"\n",
    "\n",
    "    - `N0`: Mean maximum total number of photons, default=10\n",
    "    - `stat_root`: Path to precomputed data, default=\"./stat/\"\n",
    "    - `arch`: Choose among 'dc-net','pinv-net', 'upgd', default=\"dc-net\"\n",
    "    - `denoi`: Choose among 'cnn','cnnbn', 'unet', default=\"unet\"\n",
    "\n",
    "- Specific models parameters\n",
    "    - `upgd_iter`: Number of unrolled iterations for UPGD, default=6\n",
    "    - `upgd_lamb`: Initial step size parameters for UPGD, default=1e-5\n",
    "\n",
    "\n",
    "- Optimisation:\n",
    "    - `num_epochs`: Number of training epochs, default=30\n",
    "    - `batch_size`: Size of each training batch, default=512\n",
    "    - `reg`: Regularisation Parameter, default=1e-7\n",
    "    - `step_size`: Scheduler Step Size, default=10\n",
    "    - `gamma`: Scheduler Decrease Rate, default=0.5\n",
    "    - `checkpoint_model`: Optional path to checkpoint model, default=\"\"\n",
    "    - `checkpoint_interval`: Interval between saving model checkpoints, default=0\n",
    "    - Training is done with *Adam* optimizer, *MSELoss*\n",
    "\n",
    "- Tensorboard:\n",
    "    - `tb_path`: Relative path for Tensorboard experiment tracking logs, default=False\n",
    "    - `tb_prof`: Code profiler with Tensorboard, default=False\n",
    "    - Logging of scalars *train_loss*, *val_loss* and images (dataset example ground-truth and predictions at different epochs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, data is perturbed by Poisson noise (100 mean photons) and undersampling factor of 4, on stl10 dataset, and training is done with default parameters and using experiment tracking with tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "meas = 'hadam-split'    # measurement type\n",
    "noise = 'poisson' # noise type\n",
    "prep = 'split-poisson'    # preprocessing type\n",
    "#\n",
    "N0 = 100        # ph/pixel max: number of counts\n",
    "img_size = 64   # image size\n",
    "M =  img_size**2 // 4  # Num measurements = subsampled by factor 4\n",
    "#\n",
    "data_root = './data/'\n",
    "data = 'stl10'\n",
    "stat_root = './stat'\n",
    "subs = 'var' # subsampling types: 'var': high variance, 'rect': low frequency\n",
    "#\n",
    "arch = 'dc-net' # Network architecture\n",
    "denoi = 'unet' # Denoiser architecture\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "# Tensorboard logs path\n",
    "name_run = f\"stl10_splitmeas_{subs}_{M}_{int(N0)}_{img_size}x{img_size}_{arch}_{denoi}\"\n",
    "mode_tb = True\n",
    "if (mode_tb is True):\n",
    "    if not os.path.exists(f'runs'):\n",
    "        os.mkdir(f'runs')\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    tb_path = f'runs/runs_{name_run}/{now}'\n",
    "    print(f\"Tensorboard logdir {tb_path}\")\n",
    "else:\n",
    "    tb_path = None\n",
    "    \n",
    "tb_prof = False # False\n",
    "checkpoint_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time: 2 min to download stl10, 2 min per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train.py\n",
    "if (mode_colab is True):\n",
    "    !python3 train_gen_meas.py --meas $meas --noise $noise --prep $prep --N0 $N0 --M $M --data_root $data_root --data $data --stat_root $stat_root --subs $subs --tb_path $tb_path --tb_prof $tb_prof --arch $arch --denoi $denoi --num_epochs $num_epochs --img_size $img_size --checkpoint_interval $checkpoint_interval --batch_size $batch_size\n",
    "else:\n",
    "    import subprocess\n",
    "    subprocess.run(['python3', 'train_gen_meas.py', '--meas', meas, '--noise', noise, '--prep', prep,\n",
    "                '--data_root', data_root, '--data', data, '--stat_root', stat_root,\n",
    "                '--N0', str(N0), '--M', str(M), '--subs', subs, '--img_size', str(img_size),\n",
    "                '--arch', arch, '--denoi', denoi, '--num_epochs', str(num_epochs),\n",
    "                #'--upgd_iter', str(upgd_iter),\n",
    "                '--tb_path', tb_path,\n",
    "                '--checkpoint_interval', str(checkpoint_interval, '--batch_size', str(batch_size))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch tensorboard. Select *SCALARS* or *IMAGES*. More options are available in the top-right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "# %tensorboard --logdir $tb_path\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run twice tensorboard\n",
    "#!lsof -i:6006\n",
    "#!kill -9 17387"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
