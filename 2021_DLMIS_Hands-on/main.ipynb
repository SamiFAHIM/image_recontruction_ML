{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Deep neural network under-sampled image reconstruction for X-Ray tomography**\n",
    "*PyTorch 1.7; spyrit 0.13.5*\n",
    "\n",
    "Authors: N Ducros, T Leuliet, A Lorente Mur, Louise Friot-Giroux\n",
    "    \n",
    "Cntact: *nicolas.ducrosr@creatis.insa-lyon.fr*\n",
    "    \n",
    "##  <span style=\"color:brown\"> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spyrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import fht\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.io as sio\n",
    "import scipy.linalg as lin\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import model_Radon_DCAN as model_radon\n",
    "import h5py as h5\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from skimage.transform import radon, rescale\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Acquisition\n",
    "img_size = 64 # image size\n",
    "pixel_size = 64 #Number of pixels of the sensor\n",
    "\n",
    "#- Using CPU or GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **1 - Computed Tomography (CT) and Radon operator**\n",
    "\n",
    "## Computed Tomography (CT)\n",
    "Computed tomography (CT) is an imaging modality that reconstructs 2D or 3D objects from attenuation measurements. CT is a technique used in non-destructive inspection but most notably in medical imaging, where attenuation allows the type of tissue (e.g., bone, soft tissue) and structures (e.g., tumors) to be identified. The image formation process can be modelled by the Radon transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Radon transform\n",
    "The [Radon transform](https://en.wikipedia.org/wiki/Radon_transform) is an integral transform that returns line integrals over hyperplanes (e.g., along lines for a 2D object). We illustrate this process in a discrete setting below. We consider the projection of a discrete object image $\\mathbf{x}$ along a projection ray $(r_j, \\theta_k)$, where $\\{r_j\\}$ is the detector pixel locations and $\\theta_k$ the projection angle. The integral  measured for all detector pixels under all projection views is known as the '[sinogram](https://en.wikipedia.org/wiki/Radon_transform#/media/File:Radon_transform_sinogram.gif)'. \n",
    "\n",
    "<img src=\"fig/tomo.png\" alt=\"Projections schem\" style=\"width: 80%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: How does the sinogram of a point object (i.e., an image with only one nonzero pixel) look like? Complete the code below**\n",
    "    \n",
    "<font color='green'>**Help: Create a (img_size x img_size) image with only one pixel set to 1. Set all the other pixels to 0.**</font>\n",
    "\n",
    "<font color='red'>**A: See the plot**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector with acquired angles\n",
    "theta = np.linspace(0., 180., 181)\n",
    "\n",
    "#Creation of an object image with a single activated pixel\n",
    "example = np.zeros((img_size, img_size)) #COMPLETE\n",
    "example[5,5] = 1 # -----------------------DELETE HERE\n",
    "\n",
    "#Simulation of scan via radon function\n",
    "sinogram = radon(example, theta, circle=False)\n",
    "sinogram = rescale(sinogram, scale=(pixel_size/sinogram.shape[0],1), mode='reflect', multichannel=False)\n",
    "\n",
    "#Plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax1.set_title(\"Object image\")\n",
    "ax1.imshow(example, cmap=plt.cm.Greys_r)\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Dectector pixel\")\n",
    "ax2.imshow(sinogram, cmap=plt.cm.Greys_r,\n",
    "           extent=(0, 180, 0, sinogram.shape[0]), aspect='auto')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Explain the 'sinogram' terminology.**</font>\n",
    "\n",
    "<font color='red'> **A: The point response of the Radon transform is a sine. Therefore, any sinogram can be seen as the weighted sum of many sines, each of them originating from one of the pixels in the object image.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> The Radon operator\n",
    "The Radon transform is a linear operator. Therefore, the sinogram $\\textbf{m}$ can be simply obtained as the matrix vector product $\\textbf{m} = \\textbf{Af}$, where $\\textbf{A}$ represent the discrete Radon (forward) operator and $\\textbf{f}$ is the object image. Both $\\textbf{m}$ and $\\textbf{f}$ are column vectors; $\\textbf{m}$ contains all the measurements, $\\textbf{f}$ all the image pixels. $\\textbf{A}$ is a matrix, whose dimensions match with the dimentions of $\\textbf{m}$ and $\\textbf{f}$.\n",
    "\n",
    "This is illustrated below. \n",
    "    \n",
    "<img src=\"fig/def var.jpg\" alt=\"m and A\" style=\"width: 50%;\"/> <img src=\"fig/dim.jpg\" alt=\"forward operator\" style=\"width: 49%;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Creating the operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective here is to create this forward operator $A$ for a toy example of images of size $16\\times 16$. \n",
    "\n",
    "<font color='blue'>Q: **Based on the explanation above, complete the code below to create the matrix 'A_example'**</font>.\n",
    "\n",
    "<font color='green'>**Help: First, determine the size of the Radon matrix**</font>\n",
    "\n",
    "<font color='green'>**Help: Next, generate one column of the Radon matrix at a time using the 'radon' function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of angles of acquisition\n",
    "total_angles = 181\n",
    "\n",
    "# Define an empty matrix A\n",
    "img_size_example = 16 #COMPLETE\n",
    "pixel_size_example = 16 #COMPLETE\n",
    "A_example = np.zeros((pixel_size_example*total_angles, img_size_example*img_size_example)) #COMLPLETE\n",
    "\n",
    "# Build the forward operator, one column at a time. \n",
    "for i in range(img_size_example):\n",
    "    for j in range(img_size_example):\n",
    "        # Activating a single pixel of the object image\n",
    "        image = np.zeros((img_size_example,img_size_example)) ##COMPLETE\n",
    "        image[i,j] = 1 ##COMLPETE\n",
    "        \n",
    "        # Radon transform\n",
    "        sinogram = radon(image, theta, circle=False) ##COMPLETE\n",
    "        sinogram = rescale(sinogram, scale=(pixel_size_example/sinogram.shape[0],1), mode='reflect', multichannel=False)\n",
    "        ## COMPLETE above\n",
    "        #Juxtaposing results in A matrix \n",
    "        A_example[:,pixel_size_example*i+j] = np.reshape(sinogram, (pixel_size_example*total_angles, )) ##COMLPETE\n",
    "\n",
    "# A matrix visualisation\n",
    "fig, ax = plt.subplots(figsize=(100, 2))\n",
    "ax.imshow(np.transpose(A_example))\n",
    "ax.set_title(\"A operator\")\n",
    "ax.set_xlabel(\"Sinogram projection ray\")\n",
    "ax.set_ylabel(\"Object image pixel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How does the Radon matrix $\\mathbf{A}$ look like? Especially, why is it sparse?**\n",
    "</font>\n",
    "\n",
    "<font color='red'> **Each row corresponds to one pixel position in the image domain. Each column corresponds to one projection ray in the sinogram. The matrix is sparse because only few pixels in the object image contribute to the measurement obtained for a given detector pixel under a given projection angle. For instance, at projection angle 0, there are only 16 pixels in the image that contribute to the measurement in each detector pixel, which we can observe with the \"lines\" in the forward matrix.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Testing the resulting matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Check that the created matrix matches the Radon transform. For this, compare sinograms obtained with the radon function and using the matrix-vector product**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = shepp_logan_phantom()\n",
    "phantom = rescale(phantom, scale=(img_size_example/phantom.shape[0]), mode='reflect', multichannel=False)\n",
    "\n",
    "# Radon transform with skimage function\n",
    "radon1 = radon(phantom, theta, circle=False) ##COMPLETE\n",
    "radon1 = rescale(radon1, scale=(pixel_size_example/radon1.shape[0], 1), mode='reflect', multichannel=False)\n",
    "## COMPLETE ABOVE\n",
    "\n",
    "# Radon transform with A matrix - Maybe do with numpy???\n",
    "radon2 = torch.mv(torch.Tensor(A_example),torch.flatten(torch.tensor(phantom).float()))\n",
    "radon2 = radon2.view(img_size_example, 181)\n",
    "\n",
    "# Plots ##COMPLETE THE PLOT\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax1.set_title(\"Sinogram w Radon transform\")\n",
    "ax1.set_xlabel(\"Projection angle (deg)\")\n",
    "ax1.set_ylabel(\"Projection position (pixels)\")\n",
    "ax1.imshow(radon1, cmap=plt.cm.Greys_r, extent=(0, 180, 0, radon1.shape[0]), aspect='auto')\n",
    "\n",
    "ax2.set_title(\"Sinogram w matrix $A$\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "ax2.imshow(radon2, cmap=plt.cm.Greys_r, extent=(0, 180, 0, radon2.shape[0]), aspect='auto')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Inverse matrix\n",
    "We now aim at retrieving $\\textbf{f}$ from the measurements $\\textbf{m}$. \n",
    "    \n",
    "A basic idea could be to invert the matrix $\\textbf{A}$. However, the matrix is not square and is badly [conditioned](https://en.wikipedia.org/wiki/Condition_number). Therefore, we will consider the Moore-Penrose [pseudo inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), the most widely known generalization of the inverse matrix.\n",
    "     \n",
    "The pseudo-inverse of a matrix $\\textbf{A}$, denoted $\\textbf{A}^\\dagger$, is the matrix such that $\\tilde{\\textbf{f}} = \\textbf{A}^\\dagger \\textbf{m}$ solves the problem of inverting $\\textbf{m} = \\textbf{Af}$ in the least squares sense.\n",
    "\n",
    "<img src=\"fig/ill_pinv.png\" alt=\"m and A\" style=\"width: 100%;\"/>    \n",
    "    \n",
    "<!-- It can be shown that if $Q_1 \\Sigma Q_2^T = A$ is the singular value decomposition of A, then $A^\\dagger = Q_2 \\Sigma^+ Q_1^T$, where $Q_{1,2}$ are orthogonal matrices, $\\Sigma$ is a diagonal matrix consisting of $A$’s so-called singular values (followed typically by zeros), and $\\Sigma^\\dagger$ is the diagonal matrix consisting of the reciprocals of $A$’s singular values (again, followed by zeros). [1]\n",
    "\n",
    "[1] G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pp. 139-142. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Complete the code to reconstruct $\\textbf{f}$ from $\\textbf{m}$ by computing the least square solution using two different methods that you will compare**</font>\n",
    "\n",
    "<font color='green'>**Help: First, use the pseudo inverse of $\\textbf{A}$, which can be computed using this sciPy [function](https://docs.scipy.org/doc//numpy-1.14.1/reference/generated/numpy.linalg.pinv.html).**</font>\n",
    "\n",
    "<font color='green'>**Help: Next, use a linear solver to invert the system $\\textbf{m} = \\textbf{Af}$. See for instance, this sciPy [function](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.linalg.lstsq.html)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the computed sinogram\n",
    "sinogram = np.reshape(radon1, (-1, 1))\n",
    "\n",
    "# Compute the pseudoinverse\n",
    "t0 = time.perf_counter()\n",
    "pinv = lin.pinv(A_example) # COMPLETE\n",
    "t0 = time.perf_counter() - t0\n",
    "print(t0)\n",
    "\n",
    "# Reconstruct with pseudoinverse \n",
    "t1 = time.perf_counter()\n",
    "rec_pi = np.reshape(np.dot(pinv, sinogram), (img_size_example,img_size_example)) # COMPLETE\n",
    "t1 = time.perf_counter() - t1\n",
    "\n",
    "# Reconstruct with a linear solver\n",
    "t2 = time.perf_counter()\n",
    "rec_solv = np.reshape(lin.lstsq(A_example, sinogram)[0], (img_size_example,img_size_example)) # COMPLETE\n",
    "t2 = time.perf_counter() - t2\n",
    "\n",
    "# Display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4.5))\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax1.imshow(phantom, cmap=plt.cm.Greys_r, extent=(0, 180, 0, phantom.shape[0]), aspect='auto')\n",
    "\n",
    "ax2.set_title(f'Recon using pseudoinverse \\n Time : {t0:.3f} + {t1:.3f} s')\n",
    "ax2.imshow(rec_pi, cmap=plt.cm.Greys_r, extent=(0, 180, 0, rec_pi.shape[0]), aspect='auto')\n",
    "\n",
    "ax3.set_title(f'Reco using solver \\n Time : {t2:.3f} s')\n",
    "ax3.imshow(rec_solv, cmap=plt.cm.Greys_r, extent=(0, 180, 0, rec_solv.shape[0]), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Which approach is faster? When should we use one or the other?**</font>\n",
    "\n",
    "\n",
    "<font color='red'>**A: reconstruction with the solver is much faster (e.g., ~100 ms) than with the pseudo inverse (e.g., ~800 ms). However, once the pseudo inverse is computed, it can be used to reconstruct new data in a very short time (e.g., few milliseconds), while the solver will be significantly slower (e.g., ~100 ms).**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **2 - Limited-angle acquisition and reconstruction**\n",
    "\n",
    "## <span style=\"color:brown\"> Forward operator \n",
    "\n",
    "To limit the acquisition time, it may be desirable to acquire only some of the projection rays (i.e, reduce the number of projection angles or detector pixels). We investigate here the behaviour of the pseudoinverse reconstruction for limite-angle acquisition. The limited-angle forward operator can be obtained by discarding some of the rows of the full forward operator, as illustrated below.\n",
    "    \n",
    "<img src=\"fig/Explain_a_reduced.PNG\" alt=\"m and A\" style=\"width: 80%;\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will consider images of size $64 \\times 64$. In the code below, we will consider the acquisition of 20 projection angles only. We first load the full forward matrix that has been pre-computed. Only some of the rows of the full are kept to build the limited-angle forward operator 'A_reduced'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_root = '/floyd/input/dlmis21_reconstruction_hands_on_vf/'\n",
    "saved_data = data_root + 'matrices/'\n",
    "\n",
    "# Load forward matrix with full angle data\n",
    "radon_matrix_path = saved_data + 'Q{}_D{}.mat'.format(img_size, pixel_size)\n",
    "H = sio.loadmat(radon_matrix_path)\n",
    "A = H.get(\"A\")\n",
    "A = np.array(A)\n",
    "A = torch.from_numpy(A)\n",
    "A = A.type(torch.FloatTensor)\n",
    "\n",
    "# Load the corresponding pseudoinverse\n",
    "pinv_matrix_path = saved_data + 'pinv_Q{}_D{}.mat'.format(img_size, pixel_size)\n",
    "H = h5.File(pinv_matrix_path, 'r')\n",
    "pinvA = H.get(\"A_pinv\")\n",
    "pinvA = np.array(pinvA)\n",
    "pinvA = np.transpose(pinvA)\n",
    "pinvA = torch.from_numpy(pinvA)\n",
    "pinvA = pinvA.type(torch.FloatTensor)\n",
    "\n",
    "# Compute the reduced forward matrix\n",
    "nbAngles = 20\n",
    "Areduced = model_radon.radonSpecifyAngles(A, model_radon.generateAngles(nbAngles))\n",
    "Areduced = Areduced.type(torch.FloatTensor)\n",
    "print(A.shape) # COMPLETE\n",
    "print(Areduced.shape) # COMPLETE\n",
    "\n",
    "# Compute the corresponding pseudoinverse\n",
    "if device == \"cuda:0\":\n",
    "    pinvAreduced = lin.pinv(Areduced.cpu().numpy())\n",
    "else:\n",
    "    pinvAreduced = lin.pinv(Areduced.numpy())\n",
    "    \n",
    "pinvAreduced = torch.from_numpy(pinvAreduced)\n",
    "pinvAreduced = pinvAreduced.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: What are the dimensions of the limited-angle forward operator A_reduced ?**</font>\n",
    "\n",
    "<font color='red'>**A: A_reduced is of size (64x64, 20x64)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: Are A and A_reduced still numpy arrays? What is their type? Why changing?**</font>\n",
    "\n",
    "<font color='red'>**A: We now manipulate them as torch tensors for integrating them into neural networks**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the quality of the reconstructions obtained from full-angle and limited-angle measurements.\n",
    "\n",
    "<font color='blue'> **Q: Complete the code below to** </font>\n",
    "- <font color='blue'> Compute the sinograms with both forward operators</font>\n",
    "- <font color='blue'> Reconstruct the image with the corresponding pseudoinverse. </font>\n",
    "\n",
    "<font color='green'> **Help: You can use the torch [matrix-vector mutiplication](https://pytorch.org/docs/stable/generated/torch.mv.html)** </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "im = Image.open(\"fig/image.png\")\n",
    "im = ImageOps.grayscale(im)\n",
    "\n",
    "# Preprocess the image\n",
    "im_array = np.asarray(im)\n",
    "im_array = im_array.astype(np.float32)\n",
    "im_array = 2*(im_array)/255 - np.ones([64,64])\n",
    "\n",
    "# Conversion of object image to torch tenser\n",
    "f = torch.from_numpy(im_array)\n",
    "f = f.view(1,img_size**2);\n",
    "f = f.t()\n",
    "f = f.type(torch.FloatTensor)\n",
    "\n",
    "# Simulate the measurements with full angle and limited angle configurations\n",
    "m_reduced = torch.mv(Areduced,f[:,0]) ##COMPLETE\n",
    "m_perfect = torch.mv(A,f[:,0])        ##COMPLETE\n",
    "\n",
    "# Full-angle reconstruction\n",
    "f_perfect = torch.mv(pinvA, m_perfect) ##COMPLETE #should be a 1D vector here\n",
    "# Resize to a 2D shape\n",
    "f_perfect_array = model_radon.vector2matrix(f_perfect, [img_size,img_size])\n",
    "f_perfect_array = np.transpose(f_perfect_array)\n",
    "\n",
    "# Limited angle reconstruction\n",
    "f_reconstruct = torch.mv(pinvAreduced, m_reduced) ##COMPLETE\n",
    "# Resize to a 2D shape\n",
    "f_reconstruct_array = model_radon.vector2matrix(f_reconstruct, [img_size,img_size])\n",
    "f_reconstruct_array = np.transpose(f_reconstruct_array)\n",
    "\n",
    "# Display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 4.5))\n",
    "\n",
    "ax1.set_title(\"Input image\")\n",
    "pcm1 = ax1.imshow(im_array, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "m_perfect_array = model_radon.vector2matrix(m_perfect, [total_angles,pixel_size])\n",
    "pcm2 = ax2.matshow(m_perfect_array, cmap='gray')\n",
    "\n",
    "ax3.set_title(\"Reconstructed image with 181 angles measured\")\n",
    "pcm3 = ax3.matshow(f_perfect_array, cmap='gray')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "fig.colorbar(pcm1,ax=ax1)\n",
    "fig.colorbar(pcm2,ax=ax2)\n",
    "fig.colorbar(pcm3,ax=ax3)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig2, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 4.5))\n",
    "\n",
    "ax1.set_title(\"Input image\")\n",
    "pcm1 = ax1.matshow(im_array, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "m_array = model_radon.vector2matrix(m_reduced, [nbAngles,pixel_size])\n",
    "pcm2 = ax2.matshow(m_array, cmap='gray')\n",
    "\n",
    "ax3.set_title(\"Reconstructed image with 20 angles measured\")\n",
    "pcm3 = ax3.matshow(f_reconstruct_array, cmap='gray')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "fig2.colorbar(pcm1,ax=ax1)\n",
    "fig2.colorbar(pcm2,ax=ax2)\n",
    "fig2.colorbar(pcm3,ax=ax3)\n",
    "fig2.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How do the full-angle and limited-angle reconstrcutions compare?**</font>\n",
    "\n",
    "<font color='red'>**A: Many artefacts are present; most details are lost. The pseudo inverse cannot be used for limited-angle reconstruction**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Influence of the number of angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the degradation of the image when reducing the number of measurement angles. To further study this degradation process, we will now repeat the steps for different numbers of projection angles.\n",
    "\n",
    "<font color='blue'> Run the code below, which takes about 60 seconds</font>\n",
    "\n",
    "For computational reasons, results with 100, 140 and 180 angles were pre-computed and can be compared with the obtained reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listAngles = [5, 10, 20, 40, 60]\n",
    "\n",
    "print('Computation is on going, it might take up to 1 minute...')\n",
    "##PERFORM THE WHOLE LOOP\n",
    "for ang in listAngles:\n",
    "    # Compute Areduced\n",
    "    Areduced = model_radon.radonSpecifyAngles(A, model_radon.generateAngles(ang))\n",
    "    Areduced = Areduced.type(torch.FloatTensor)\n",
    "    \n",
    "    # Compute the pseudoinverse\n",
    "    if device == \"cuda:0\":\n",
    "        pinvAreduced = lin.pinv(Areduced.cpu().numpy())\n",
    "    else:\n",
    "        pinvAreduced = lin.pinv(Areduced.numpy())\n",
    "    pinvAreduced = torch.from_numpy(pinvAreduced)\n",
    "    pinvAreduced = pinvAreduced.type(torch.FloatTensor)\n",
    "    \n",
    "    # Simulate the measurements\n",
    "    m = torch.mv(Areduced,f[:,0])\n",
    "    \n",
    "    # Reconstruct the image\n",
    "    f_reconstruct = torch.mv(pinvAreduced,m)\n",
    "    \n",
    "    # Reshape into a 2D mage\n",
    "    f_reconstruct_array = model_radon.vector2matrix(f_reconstruct, [img_size,img_size])\n",
    "    f_reconstruct_array = np.transpose(f_reconstruct_array)\n",
    "    \n",
    "    # Display results\n",
    "    plt.imshow(f_reconstruct_array, cmap='gray')\n",
    "    plt.title(f'Reconstruction w {ang} angles measured')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Display the pre-computed reconstructions\n",
    "supp_angles = [100, 140, 180]\n",
    "path_rec = data_root + 'reconstructed/'\n",
    "for ang in supp_angles:\n",
    "    f_reconstruct_array = np.load(path_rec + f'reconstructed_{ang}_angles.npy')\n",
    "    plt.imshow(f_reconstruct_array, cmap='gray')\n",
    "    plt.title(\"Reconstruction w {} angles measured\".format(ang))\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: What is the minimum number of measurement angles that allows to use the pseudoinverse as a satisfying reconstruction method?** </font>\n",
    "\n",
    "<font color='red'>**A: 60 starts to be OK**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **3 - Deep image reconstruction**\n",
    "## <span style=\"color:brown\"> Framework\n",
    "    \n",
    "Deep image reconstruction aims to design a non-linear mapping (i.e., neural network) $\\mathcal{G}_{\\omega}$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{G}_{\\omega}(\\mathbf{m}) \\approx \\mathbf{f},\n",
    "\\label{eq:mapping} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\omega$represents the parameters of the network. The parameters are optimized during the training phase to minimize the cost function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\omega^* = \\underset{\\omega}{\\text{arg min}} \\sum_{\\ell=0}^{S-1} \\| \\mathcal{G}_\\omega(\\mathbf{m}^{(\\ell)}) - \\mathbf{f}^{(\\ell)}\\|^2_2 + \\mathcal{R}(\\omega),\n",
    "\\label{eq:fn} \\tag{2}\n",
    "\\end{equation*}\n",
    "where $(\\mathbf{m}^{(\\ell)} , \\mathbf{f}^{(\\ell)})$, $0 \\le \\ell \\le L-1$ are the measurement-image pairs of the training database, and $\\mathcal{R}$ is a regularization function that stabilizes training. For this study we will consider $\\mathcal{R}(.) = \\alpha \\|.\\|^2_2$, where $\\alpha$ is a positive constant that will impact how important $\\mathcal{R}$ is with respect to the rest of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Network architecture\n",
    "\n",
    "As illustrated below, we choose to map the sinogram $\\mathbf{m}$ into the image domain (see $\\tilde{\\mathbf{f}}$) to benefit convolutional layers are particularly powerful for image denoising and artefact correction.\n",
    "\n",
    "<img src=\"fig/network.png\" alt=\"m and A\" style=\"width: 70%;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Why do we map the sinogram into the image domain before applying convolutional layers? Why do we use the Moore-Penrose pseudo-inverse rather than learning this mapping?**</font>\n",
    "\n",
    "<font color='red'>**A: First, convolutional layers have shown great success in exploiting spacial redundancies of natural images, so we would like to apply these layers in the image domain. While several studies have shown that convolutional layers can be used on sinograms too, image-domain processing is a safe choice. Second, learning the measurement to image domain mapping, would require learning a lot of parameters (i.e, $D\\times \\theta \\times, Q^2$ parameters) and the resulting network may be more sensitive to noise.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyrit.learning.model_Had_DCAN import Weight_Decay_Loss\n",
    "from spyrit.learning.nets import train_model\n",
    "\n",
    "\n",
    "net_types = ['c0mp', 'comp','pinv', 'free']\n",
    "net_arch = 2\n",
    "regularisation = 1e-7\n",
    "\n",
    "num_epochs=3\n",
    "batch_size=256\n",
    "reg=1e-7\n",
    "lr=1e-3\n",
    "step_size=20\n",
    "gamma=0.2\n",
    "checkpoint_model=\"\"\n",
    "checkpoint_interval=0\n",
    "model_root='./models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.functional.to_grayscale,\n",
    "     transforms.Resize((img_size, img_size)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=data_root+\"train\", transform=transform)\n",
    "trainloader = \\\n",
    "    torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=data_root+\"test\", transform=transform)\n",
    "testloader = \\\n",
    "    torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "dataloaders = {'train': trainloader, 'val': testloader}\n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "print(inputs.shape)  # COMPLETE\n",
    "\n",
    "im_tensor = torch.from_numpy(im_array)\n",
    "m = torch.mv(Areduced,f[:,0])\n",
    "test_batch = 1\n",
    "color = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: What is the shape of the database 'input' variable? What does each dimension correspond to?**</font>\n",
    "\n",
    "<font color='red'>**A: \\[Batch_size, channels, height, width\\]. We manipulate batches that contain 256 images of size 64 x 64; 'channels' = 1 indicates that images are grascale.**</font>\n",
    "\n",
    "<font color='blue'>**Q: Explain the line 'transforms.Normalize([0.5], [0.5])'. Why is it important?**</font>\n",
    "\n",
    "<font color='red'>**A: We normalize the data to 0 mean and unit variance. Considering normalized data stabilizes and accelerates the optimization of the parameters of the network. Normalisation is also fondamental when the raw data extend over several orders of magnitude.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Training a reconstruction network from scratch (for 3 epochs)\n",
    "\n",
    "<font color='blue'>**Q: Complete the code to compute the forward operator 'Areduced' (20 projections) and the corresponding pseudo-inverse matrix 'pinvAreduced'**</font>\n",
    "\n",
    "<font color='green'>**Help: you have already done done this in some of previous cells. The point here is to understand how the pseudo inverse is plugged into the deep reconstruction network.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amt = 4\n",
    "nbAngles = 20\n",
    "\n",
    "###################  TO BE COMPLETED\n",
    "Areduced = model_radon.radonSpecifyAngles(A, model_radon.generateAngles(nbAngles))\n",
    "Areduced = Areduced.type(torch.FloatTensor)\n",
    "if device == \"cuda:0\":\n",
    "    pinvAreduced = lin.pinv(Areduced.cpu().numpy())\n",
    "else:\n",
    "    pinvAreduced = lin.pinv(Areduced.numpy())\n",
    "pinvAreduced = torch.from_numpy(pinvAreduced)\n",
    "pinvAreduced = pinvAreduced.type(torch.FloatTensor)\n",
    "####################################\n",
    "\n",
    "model = model_radon.compNet(img_size, pixel_size, nbAngles, A = Areduced, pinvA = pinvAreduced, variant=net_arch)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "loss = nn.MSELoss();\n",
    "criterion = Weight_Decay_Loss(loss);\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr);\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model, train_info = train_model(model, criterion, \\\n",
    "        optimizer, scheduler, dataloaders, device, model_root, num_epochs=num_epochs,\\\n",
    "        disp=True, do_checkpoint=checkpoint_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Complete the code below to compare the ground truth to the Moore-Penrose pseudo-inverse, and the deep neural reconstuctor.**</font>\n",
    "\n",
    "<font color='green'>**Tip1: Remember the dimension of 'input' from the database.**</font>\n",
    "\n",
    "<font color='green'>**Tip2: Torch tensors may be in the memory of the GPU**</font>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(test_amt):\n",
    "    # Choosing random image in STL10\n",
    "    i_test = np.random.randint(0, inputs.shape[0])\n",
    "    \n",
    "    # Plots\n",
    "    fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "    fig.suptitle('', fontsize=16)\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Ground-truth\")\n",
    "    aff = ax.imshow(inputs[i_test, 0, :, :].cpu(), cmap='gray') #Complete here\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Radon pinvNet \")\n",
    "    rec = model.evaluate_fcl(inputs)\n",
    "    aff = ax.imshow(rec[i_test, 0, :, :].cpu(), cmap='gray') #Complete here\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Corrected image\")\n",
    "    rec = model.evaluate(inputs)\n",
    "    aff = ax.imshow(rec[i_test, 0, :, :].cpu(), cmap='gray') #Complete here\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: How do the pseudo inverse solution and network output compare?**</font>\n",
    "\n",
    "<font color='red'>**A: After only a few epochs, the artefacts of the pseudo inverse solution are significantly reduced. However, the output of the networks is oversmooth**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Testing a trained model (REPRENDRE ICI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = ['c0mp', 'comp','pinv', 'free']\n",
    "net_arch = 2\n",
    "num_epoch = 100\n",
    "list_angles = np.array([20, 40, 60])\n",
    "learning_rate = 1e-3\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "batch_size = 1000\n",
    "regularisation = 1e-7\n",
    "\n",
    "\n",
    "\n",
    "test_amt = 4\n",
    "nbAngles = 20\n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Deducing model file name\n",
    "suffix = '_Q_{}_D_{}_T_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\\\n",
    "               img_size, pixel_size, nbAngles, num_epoch, learning_rate, step_size,\\\n",
    "               gamma, batch_size, regularisation)\n",
    "# title = 'nets/NET_'+ net_types[net_arch] + suffix\n",
    "title = data_root + 'nets/NET_'+ net_types[net_arch] + suffix\n",
    "\n",
    "# Loading model\n",
    "model = model_radon.compNet(img_size, pixel_size, nbAngles, variant=net_arch)\n",
    "model = model.to(device)\n",
    "model_out_path = \"{}.pth\".format(title)\n",
    "model.load_state_dict(torch.load(model_out_path, map_location=torch.device('cpu')))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(test_amt):\n",
    "    # Choosing random image in STL10\n",
    "    i_test = np.random.randint(0, inputs.shape[0])\n",
    "    \n",
    "    # Plots\n",
    "    fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "    fig.suptitle('', fontsize=16)\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Ground-truth\")\n",
    "    aff = ax.imshow(inputs[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Radon pinvNet \")\n",
    "    rec = model.evaluate_fcl(inputs)\n",
    "    aff = ax.imshow(rec[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Corrected image\")\n",
    "    rec = model.evaluate(inputs)\n",
    "    aff = ax.imshow(rec[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: What is the impact of training a deep neural network until it converges?**</font>\n",
    "\n",
    "<font color='red'>**A: We can observe that the background seems to possess many fewer artifacts than before, the images while fairly smooth manage to retain many more details than it previously did.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Comparing neural Netoworks for several numbers of projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(list_angles.size):\n",
    "    print(\"Acquisition of \" + list_angles[index].astype(str) + \" angles\")\n",
    "    # Deducing model file name\n",
    "    suffix = '_Q_{}_D_{}_T_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\\\n",
    "                   img_size, pixel_size, list_angles[index], num_epoch, learning_rate, step_size,\\\n",
    "                   gamma, batch_size, regularisation)\n",
    "    # title = 'nets/NET_'+ net_types[net_arch] + suffix\n",
    "    title = data_root + 'nets/NET_'+ net_types[net_arch] + suffix\n",
    "    \n",
    "    # loading model\n",
    "    model = model_radon.compNet(img_size, pixel_size, list_angles[index], variant=net_arch)\n",
    "    model = model.to(device)\n",
    "    model_out_path = \"{}.pth\".format(title)\n",
    "    model.load_state_dict(torch.load(model_out_path, map_location=torch.device('cpu'))) \n",
    "    model_bis = model;\n",
    "    \n",
    "    # Plots\n",
    "    fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "    fig.suptitle('', fontsize=16)\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Sinograme\")\n",
    "    rec = model.forward_acquire(inputs, test_batch, color, img_size, img_size)\n",
    "    rec_array = model_radon.vector2matrix(rec[0, :, 0, 0].cpu(), [list_angles[index], 64])\n",
    "    aff = ax.imshow(rec_array, cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Radon pinvNet \")\n",
    "    rec = model.evaluate_fcl(inputs)\n",
    "    aff = ax.imshow(rec[0, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Corrected image\")\n",
    "    rec = model.evaluate(inputs)\n",
    "    aff = ax.imshow(rec[0, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Conclude on the limits of Deep-neural reconstructors.**</font>\n",
    "\n",
    "\n",
    "<font color='red'>**A: When the data is heavily down-sampled, it then even deep-neural reconstructors cannot overcome the resulting artefacts. The students may understand that even imperfect reconstructions are the best one can obtain sometimes.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Conclusion**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this hands on session, you should be able to :\n",
    "- Understand the importance of modeling the forward operator of an inverse problem.\n",
    "- Reconstruct an image from measurement data thanks to linear reconstructors such as the Moore-Penrose pseudo-inverse.\n",
    "- Reconstruct an image from measurement data thanks to non-linear reconstructors such as convolutional neural networks : the impact of training for a small number of epochs, and how to integrate linear reconstructors to deep-learning methods.\n",
    "- Understand the limits of reconstruction methods when the data is undersampled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
